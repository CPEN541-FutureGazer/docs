%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,authordraft]{acmart}
%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen,review]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title{Are You Looking at Me? Eye Gazing in Web Video Conferences}

\author{Muchen He}
\affiliation{%
  \institution{University of British Columbia}
  \city{Vancouver}
  \country{Canada}}
\email{mhe@ece.ubc.ca}

\author{Beibei Xiong}
\affiliation{%
  \institution{University of British Columbia}
  \city{Vancouver}
  \country{Canada}}
\email{bear233@student.ubc.ca}

\author{Kaseya Xia}
\affiliation{%
  \institution{University of British Columbia}
  \city{Vancouver}
  \country{Canada}}
\email{zxia0101@student.ubc.ca}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
TODO: need to modify

The efficiency of the communication using current web video conferencing (WVC) systems is obstructed by the lack of eye contact due to the disparity between the position of the camera and the position of the eyes on the screen. There exists some high-end expensive WVC systems that can partially solve this problem, but still it is not solved for consumer-level. ...

In this paper, we describe a semi-structured interview study with 10 participants that investigates how including eye-contact in current WVC system affects user experience. (To general? Need to be more specific I guess.)

Our overall findings indicate that, …


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003120.10003121.10003122.10003334</concept_id>
<concept_desc>Human-centered computing~User studies</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003121.10003122.10010854</concept_id>
<concept_desc>Human-centered computing~Usability testing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~User studies}
\ccsdesc[300]{Human-centered computing~Usability testing}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Eye-tracking, gaze-tracking human-computer-interaction, web video conferencing, online-learning}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Web video conferencing (WVC), exacerbated by the COVID-19 pandemic, are an essential tool in remote-learning and remote-working situations. Unfortunately, the same technology has lacked innovation in push human-computer-interaction that enhances WVC user-experience, such as amplifying engagement through eye-contact.

We propose a gaze-tracking WVC system, called FutureGazer, that emulates eye-contact amongst the participants in a WVC meeting room to enable a highly interactive environment. To test our system, we intend to select students partaking in various online classes at the University of British Columbia. Figure 1 shows what we intend to build in contrast to existing WVC platforms like Zoom. Figure 2 depicts the personalized eye-contact emulation enabled by our system -- we discuss this further in Section 3. For more information, please refer to our project proposal.
In this document, we outline the design of FutureGazer, the research questions regarding user experience, and the experimental design on the collection and evaluation of user data to support our research insight.

Our project explores whether adding eye-contact to the current WVC system will enhance the sense of interaction and presence of the users. Conventional WVC services only offer standard visual and audio communication, and they do not support intuitive and personalized eye-contact between users. Therefore, people still prefer face-to-face meetings because of the highly interactive meeting environment [3].

In distant-learning classes, for example, presenters (e.g. professors, teachers, students) often feel distracted or disengaged when there are no audiovisual feedback coming from the audience. These feedback include eye contact, gaze direction, and other body language cues.
For example, in the Gallery View in Zoom (and other WVC applications), the audience consists of black boxes  and name tags, as shown in Figure 1. If the participant has video turned on, the webcam video stream replaces their box. We will refer to the space each participant takes up on the screen as their footprint. Notice that everyone has the same and uniform footprint, regardless if they are paying attention to the meeting or the active participant. 
Thus, current systems neglect important cues presenters use to moderate their lecture. The lack of interactivity is one reason why online lectures are less effective than in-person lectures [6]. In one of the first experimental studies [2] on the effects of traditional instruction versus online learning, students attend live lectures instead of watching the same lectures online while supplemental materials and instructions were the same. Researchers [2] found modest evidence that the traditional format trumps the online format in engagement. Many people also think it is odd to see their faces during conversations, and it is hard to look away — significantly distracts participants during WVCs. 
Lastly, some people are camera-shy and do not want to reveal themselves in WVCs. Thus we decide to explore the effect on participation and engagement from using an avatar as an alias instead of  a live video.
The key metrics we want to observe in this project are: participant’s attention, engagement, and the feeling of connectedness. To explore parameters that effect these metrics, we consolidate these ideas into three core research questions (hereafter will be refer to as RQ1, RQ2, and RQ3):
\newline
\begin{enumerate}
\item Can a person tell if they are being looked at in a WVC and how can 3D avatars be augmented to enhance this experience.
\item Can a person tell if other participants are looking at each other in a WVC and how using 3D avatars can be augmented to increase engagement.
\item How does a person’s attention change as the avatars augmented with WVC enables eye-contact and gaze.
\end{enumerate}

 We intend to modify design parameters to our prototype user-interface (UI) of our mock WVC program  to study the behaviour of participants. 

\section{Related Work}
TODO: 
This section provides an overview of Web Video Conferencing (WVC) and gaze tracking studies. Our work investigated the ...

\textbf{Web Video Conferencing}
\newline
WVC is a synchronous model that provides verbal and visual communication between two or more participants. Examples of WVC services include Zoom, Collaborate Ultra, Microsoft Teams, and others. When the COVID-19 pandemic emerged, and in-person classes transitioned to online-learning, researchers evaluated students’ satisfaction with WVC-based learning and social activities.

WVC generally provides a more collaborative and engaging experience for students using interactive breakout rooms [1]. Some also suggest WVC provides higher satisfaction scores than other tools and has become one of the most popular online teaching methods [7] [8]. 

However, in the study by [3], 80\% of the students felt they would be more engaged in a standard class setting, and 57\% of the students thought WVC technology is a barrier to their interaction with instructors. 

Since WVC hinders eye-contact in larger meetings, participants also observe lower attention and memory retention, a side-effect of lack of direct eye-gazes [10]. Lastly, a study observes an increase in participants’ prosocial behaviour when being watched by deceptive video conferencing manipulation [2]. 
\newline

\textbf{Gaze Tracking}
\newline

Classical gaze-tracking methods estimate where a user is looking, but these implementations require expensive hardware and are not robust across different environments and poses [4-6]. Conventional WVC services (e.g. Zoom), such as shown in Figure 1(a), offer standard audio and visual communication but lack innovation in bringing participants’ social hints such as intuitive and personalized eye-contact to the audience. NVIDIA Maxine uses GANs to infer facial expressions and reconstruct a photorealistic feed where a presenter can look in arbitrary directions. However, their implementation only ensures direct-eye contact to the screen’s centre and does not support larger meeting rooms [11]. 

The mixed reception of WVC and lack of non-verbal human interface forms the primary motivation for us to close the gap between teleconferencing and traditional F2F meetings. Moreover, we investigate the relationship between direct eye-gazing and prosocial behaviour in a WVC environment.
\newline

\textbf{Eye Contact in Current WVC Systems}
\newline
A large body of prior work has explored that eye contact is a critical aspect of human communication. [1, 2] Eye contact plays an important role in both in person and a WVC system. [3, 4] Therefore, it’s critical and necessary to preserve eye contact in order to realistically imitate real-world communication in WVC systems. However, perceiving eye contact is difficult in existing video-conferencing systems and hence limits their effectiveness. [2] The lay-out of the camera and monitor severely restricted the support of mutual gaze. Using current WVC systems, users tend to look at the face of the person talking which is rendered in a window within the display(monitor). But the camera is typically located at the top of the screen. Thus, it’s impossible to make eye contact. People who use consumer WVC systems, such as Zoom, Skype, experience this problem frequently. This problem has been around since the dawn of video conferencing in 1969 [5] and has not yet been convincingly addressed for consumer-level systems. 

Some researchers aim to solve this by using custom-made hardware setups that change the position of the camera using a system of mirrors [6,7]. These setups are usually too expensive for a consumer-level system. Software algorithms solutions have also been explored by synthesizing an image from a novel viewpoint different from that of the real camera. This method normally proceeds in two stages, first they reconstruct the geometry of the scene and in second stage, they render the geometry from the novel viewpoint. [8, 9, 10, 11, 12] Those methods usually require a number of cameras and not very practical and affordable for consumer-level. Besides, those methods also have a convoluted setup and are difficult to achieve in real-time. 

Some gaze correction systems are also proposed, targeting at a peer- to-peer video conferencing model that runs in real-time on average consumer hardware and requires only one hybrid depth/color sensor such as the Kinect. [13] However, when there are more than two persons involved in a web video conference, even with gaze corrected view, users still cannot tell whether a person is looking at him or someone else in the meeting. With the gaze correction, it will create the illusion that everyone in this meeting is looking out of the screen. This could cause a serious confusion. 

Therefore, we propose our system ….


\newline
\section{User Experiment}
\newline
\textbf{Design Overview}
\newline
Figure 3 shows the high-level dataflow of the system. Standard webcam captures video. The video data then passes to both the gaze-tracking ML model and the video conferencing client, in case the user only wants to use video. The pre-trained ML model uses open-source technology trained with open-source datasets such as WebGazer [8] and WebGazer dataset [7]. The ML model outputs the gaze-tracking metadata, such as screen-space pixel-coordinates. Using the metadata, we can determine where the user is looking and whether they are looking at the screen. The blue region in Figure 4 encapsulates most of our technical implementation inside a single program. 
There are two output pipelines: (1) local rendering and (2) remote data-forwarding. Local rendering refers to the 3D scene construction, rendering, and composition for the local user. Remote data-forwarding refers to forwarding the local metadata to other participants’ clients and letting their client handle the user’s avatar’s rendering.
In local rendering, the gaze-tracking metadata passes to the local 3D avatar renderer. The gaze’s pixel-coordinates map on to a 3D rotation of the avatar using some mapping function M. The local 3D avatar renderer also takes in the global conference metadata and remote gaze-tracking metadata. The conference metadata consists of participants’ spatial arrangements, personalization settings, and other global settings. The remote gaze-tracking metadata comes from other participants. Both conference metadata and remote gaze-tracking metadata are parameters that affect the mapping M to ensure gaze-target coherency across different perspectives — as demonstrated in Figure 2. 
Our system broadcasts the gaze-tracking metadata to other participants in the meeting/conference via WebSocket in remote data-forwarding. On their clients, this metadata is combined with the conference metadata (not shown in the figure of Figure 3) to generate a different mapping M for their screens — also ensuring gaze-target coherency.
Figure 4 depicts the high-level network dataflow in a meeting with three participants: A, B, and C.

\newline
\textbf{Participants}
\newline
We recruited 10 participants...

\newline
\textbf{Procedure}
\newline
In this section, we outline our preliminary strategy to perform user experiments and collect quantifiable data for our evaluations of how FutureGazer prototype affects user behaviour. We use an existing popular WVC application, Zoom, as our control variable in our experiments. 

\textbf{Experiment A}
\newline
Experiment A attempts to tackle C1 by studying if a person can tell if they’re being looked at, or if someone starts to look at them. 
Procedure: Begin by setting up  mock-avatars in the WVC window, each with a unique name as seen in Figure 5(a). The mock-avatars does not correspond to real users in the WVC room and are programmed and controlled prior to user testing.
The test participant joins the meeting session as the th person. Initially, the mock-avatars move randomly for  seconds (Figure 5(b)). Meanwhile an audio track of lecture or podcast plays. One of the mock-avatar, hereafter called presenter-avatar, is programmed to be synced with the audio track for the sake of realism. 
Throughout the meeting, a set of specific pre-programmed mock-avatars (that is not the presenter-avatar) will look at the test participant (look out from the screen) intermittently for  at varying frequencies without disrupting the presenter-avatar or the audio (Figure 5(c)). We call this event a stare. The participant does not know which mock-avatars are selected to look at them to preserve the validity of the experiment. We initiate a pop-up window prompting the test-participant whether they believe they’re believed to be looked. If so, which mock-avatar is looking at them. This process repeat for  more times and record the participant’s response for evaluation. 
Parameters: The number of mock-avatars in the WVC meeting room is . We believe 9 mock-avatars (not including the test participant themselves) is a good value. If  is too low then the gaze is too obvious (since the participant has few avatars to scan) too notice and we would not gain usable insights. However, if  is too high, then the participant might be too distracted and the collected data from participants might be too noisy.
The initial time between when participants join the WVC room and the first pre-programmed stare occurrence is . We set this to 20 seconds as we would like to keep the experiment short. Any shorter and the randomized mock-avatar gaze change may seem too obvious, but any longer may distract the participant from the main intent of the experiment. 

is the duration of the stare (i.e. number of seconds a mock-avatar looks at the participant). While in implementation, this value is randomized 2 seconds for realism, we think 10 seconds is a good starting point.
Evaluation: We compare and correlate the participants’ response (recorded avatar selection) and the the pre-programmed mock-avatar sequence. The correlation tests the hypothesis set in C1.
\newline
\textbf{Experiment B}
\newline
Experiment B explores C3 by observing whether a person who is paying attention to a presenter can notice another person who starts to look at them (i.e. the gaze target change to the subject).
Procedure: Similar to Experiment A, the initial setup consist of  mock-avatars in the WVC window with the participant joining as the th person. For the first  seconds, one mock-avatar (A) is talking whilst all other mock-avatars look at A. Afterwards, a different mock-avatar (B) initiates stare similar to Experiment A (i.e. starts to look at the participant).
At this time, a small line of text will show up at the top of B to ask the participant to use the mouse to click on B. We record the response time between when B initiates stare and when the participant clicks on B for evaluation. 
Parameters: Similar to Experiment A,  is the number of participants and  specifies the duration between the start of the experiment and the first stare event to allow the participant to get settled in the experiment. We choose  and  seconds.
Evaluation: The recorded response time corresponds to how fast participant’s attention is being redirected to the mock-avatar looking at them. If the response time is longer than a threshold (e.g. 10 seconds), it reflects that the participant is paying attention to the speaker avatar (A) and not paying attention to others (including B).
External Tracking Variation: In the original design of Experiment B, we prompt the participants with a line of text to ask them to click. However, this might cause bias as doing so primes the participants with extra hints that affect experiment output. Thus, the variation of Experiment B, called Experiment B-TCK, uses an external gaze tracking system to measure where the participant is looking at. 
In Experiment B-TCK, the mock-avatar setup is identical to Experiment B, except the preprogrammed stare event sequences are predetermined/logged. We use external gaze tracking systems such as tobii [9] to log participants’ actual gaze target instead of relying on the participants to actively click on the mock-avatar they think that is looking at them. For evaluation, we compare and correlate the predetermined gaze sequence of the mock-avatars with the captured gazes of the participants’ throughout the experiment. We hypothesize that during the first  seconds, the participant mostly focuses on A, but as B initiates a stare, we expect the participant to shift their gaze to B’s location on the screen.
However, due to COVID-19 pandemic remote work requirement of this project, we do not expect to carry-out Experiment B-TCK. 
\newline
\textbf{Experiment C}
\newline
Experiment C attempts to test whether the presenter can tell if the audience is paying attention to their speech and tackles both C1 and C3.
Procedure: We set up  mock-avatars in the WVC window and the participant will join the session as the th user. The participant to perform five segments of speeches/lectures to the mock-avatars. 
Each of the mock-avatars can randomly toggle between two modes: Paying attention (PA) and Not paying attention (NPA). During the experiment, we program a controller for a variable  where \% of the mock-avatars will be in PA mode and the other \% of the mock-avatars will be in NPA mode.  can also be randomized at run-time with varying frequencies. Lastly, the  value is logged throughout the experiment. 
After the participants are done talking for each segment, we ask the participant to rate whether they think they are being paying attention to. The rating is a confidence score (0-100\%) that also reflect their estimate of how many mock-avatars were actively paying attention during their speech.
Parameters: Like previous experiments, we intend to tune and also study the influence of number of people in a WVC room, . To start, we choose  to limit the participants from being distracted themselves. However, we would also like to experiment with a much larger WVC room size (e.g. ). This is to study whether a grid of avatars could create a vector-field like effect where the participants would abstract the avatars into simple vectors.
Evaluation: We compare the participants’ rating and the logged \% values of mock-avatars in PA mode. A strong correlation (>0.75) implies that C1 and C3 are likely true.
\newline
\textbf{Experiment D}
\newline
Experiment D attempts to test C2 in a small-group WVC environment as we assume eye contact amongst two or more people can incite a closer and intimate relationship to an observer [4]. Inspired by the body sheets as a method to collect user responses in La Delfa et al.’s work in Drone Chi [5], we intend to use a relationship matrix sheet to study the effect of 3D avatars in
Procedure: We set up 4 mock-avatars talking to and looking at each other with a random but preprogrammed sequence. Note that the 4 mock-avatars will not change their gaze target for the first  seconds. 
Each mock-avatar take turns (randomly assigned) talking for  seconds. Meanwhile, the other three mock-avatars who are not talking each will randomly look at each other with some set probability. Thus for each turn, we can define this probability as a matrix  which corresponds to the relationship matrix discussed earlier: 

Where  is the probability mock-avatar  is looking at (paying attention to) mock-avatar  and all columns and rows adds up to 1.0. 
When the experiment is complete and all mock-avatars finished taking turns speaking, we give the relationship matrix as shown in Figure 6 to the participant to articulate which avatar-pair is more intimate, as well as which avatar is talking with which.
Evaluation: 
We ask the participants to mark each directional arrow, as shown in Figure 6, of the relationship matrix, to indicate which avatar is engaging with which. We may also ask the participant to annotate each arrow with a confidence score (0.0 - 1.0). These scores can be normalized and compared with the probability matrix  that was preprogrammed into the mock-avatars. A strong correlation (>0.75) of participants’ response and  would imply C2 is likely true.
\textbf{Experiment E}
\newline
Experiment E does not directly test any of the core research questions but instead attempts to challenge the validity of the 3D avatars, and whether they are even necessary to communicate gaze and eye-contact in WVCs. Using one of the previous experiment as the baseline, we swap out the 3D mock-avatars with 2D eyeballs that can articulate and look around, similar to xeyes from langui.net [1]. The goal is to see whether the benefits of 3D objects, shading, and scenes influence participants’ engagement, immersion, and attention or not. 
Procedure: We repeat Experiment A, B, C, and D using an implementation like xeyes instead of 3D avatars. 
Evaluation: We perform the same evaluation method as mentioned in experiment 1-4. If the evaluation result is the asme for both implementations, then it shows that 3D avatars are not necessarily to achieve the experiences related to C1, C2, and C3. 



\section{Results}
\section{Discussions}


\section{Limitations and Future Work}
Our project uses an existing machine-learning-based gaze-tracking framework and APIs to build a proof-of-concept WVC platform. The novel contribution is to integrate gaze-tracking, real-time avatar rendering, and WVC technology to provide a better WVC experience to users, as well as to provide a platform for us to study the effects of incorporating prominent eye-contact and other facial cues in WVC. See Section 4 for the core research questions explored.
The major limitation is that the avatar may not be as realistic as the human face. So, talking to an animated head with fake eyes may not give the participants the same eye contact experience as in real life. The number of participants in the meeting is another drawback of this prototype; if there are more than 15 participants, their avatars would be arranged into more than one page. While we can overcome the arrangement issue trivially by programming a custom front-end, each participant will have a tiny grid, making the gaze-tracking component a challenge. 

\section{Conclusion}

\section{CCS Concepts and User-Defined Keywords}

Two elements of the ``acmart'' document class provide powerful
taxonomic tools for you to help readers find your work in an online
search.

The ACM Computing Classification System ---
\url{https://www.acm.org/publications/class-2012} --- is a set of
classifiers and concepts that describe the computing
discipline. Authors can select entries from this classification
system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
commands to be included in the \LaTeX\ source.

User-defined keywords are a comma-separated list of words and phrases
of the authors' choosing, providing a more flexible way of describing
the research being presented.

CCS concepts and user-defined keywords are required for for all
articles over two pages in length, and are optional for one- and
two-page articles (or abstracts).

\section{Figures}

The ``\verb|figure|'' environment should be used for figures. One or
more images can be placed within a figure. If your figure contains
third-party material, you must clearly identify it as such, as shown
in the example below.
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{sample-franklin}
  \caption{1907 Franklin Model D roadster. Photograph by Harris \&
    Ewing, Inc. [Public domain], via Wikimedia
    Commons. (\url{https://goo.gl/VLCRBB}).}
  \Description{A woman and a girl in white dresses sit in an open car.}
\end{figure}

\section{Appendices}

If your work needs an appendix, add it before the
``\verb|\end{document}|'' command at the conclusion of your source
document.

Start the appendix with the ``\verb|appendix|'' command:
\begin{verbatim}
  \appendix
\end{verbatim}
and note that in the appendix, sections are lettered, not
numbered. This document has two appendices, demonstrating the section
and subsection identification method.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Research Methods}

\subsection{Part One}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
lacinia dolor. Integer ultricies commodo sem nec semper.

\subsection{Part Two}

Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
eros. Vivamus non purus placerat, scelerisque diam eu, cursus
ante. Etiam aliquam tortor auctor efficitur mattis.

\section{Online Resources}

Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
enim maximus. Vestibulum gravida massa ut felis suscipit
congue. Quisque mattis elit a risus ultrices commodo venenatis eget
dui. Etiam sagittis eleifend elementum.

Nam interdum magna at lectus dignissim, ac dignissim lorem
rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
