@inproceedings{RN49,
   author = {La Delfa, Joseph and Baytas, Mehmet Aydin and Patibanda, Rakesh and Ngari, Hazel and Khot, Rohit Ashok and Mueller, Florian'Floyd'},
   title = {Drone chi: Somaesthetic human-drone interaction},
   booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
   pages = {1-13},
   type = {Conference Proceedings}
}


@article{RN2,
   author = {Cañigueral, Roser and Hamilton, Antonia F de C},
   title = {Being watched: Effects of an audience on eye gaze and prosocial behaviour},
   journal = {Acta psychologica},
   volume = {195},
   pages = {50-63},
   ISSN = {0001-6918},
   year = {2019},
   type = {Journal Article}
}

@article{RN3,
   author = {Doggett, Dr and Mark, Anthony},
   title = {The videoconferencing classroom: What do students think?},
   journal = {Architectural and Manufacturing Sciences Faculty Publications},
   pages = {3},
   year = {2008},
   type = {Journal Article}
}

@inbook{RN7,
   author = {Reese, Robert J and Chapman, Norah},
   title = {Promoting and evaluating evidence-based telepsychology interventions: Lessons learned from the university of Kentucky telepsychology lab},
   booktitle = {Career paths in telemental health},
   publisher = {Springer},
   pages = {255-261},
   year = {2017},
   type = {Book Section}
}

@article{RN8,
   author = {Roth, Jeffrey J and Pierce, MariBrewer, Steven},
   title = {Performance and satisfaction of resident and distance students in videoconference courses},
   journal = {Journal of Criminal Justice Education},
   volume = {31},
   number = {2},
   pages = {296-310},
   ISSN = {1051-1253},
   year = {2020},
   type = {Journal Article}
}

@article{RN10,
   author = {Hietanen, Jari K},
   title = {Affective eye contact: an integrative review},
   journal = {Frontiers in psychology},
   volume = {9},
   pages = {1587},
   ISSN = {1664-1078},
   year = {2018},
   type = {Journal Article}
}



@article{RN1,
   author = {Al-Samarraie, Hosam},
   title = {A scoping review of videoconferencing systems in higher education: Learning paradigms, opportunities, and challenges},
   journal = {International Review of Research in Open and Distributed Learning},
   volume = {20},
   number = {3},
   year = {2019},
   type = {Journal Article}
}

@misc{RN103,
   url = {https://www.processing.org},
   year = {2020},
   type = {Computer Program}
}

@article{RN42,
   author = {Hart, Paul and Svenning, Lynne and Ruchinskas, John},
   title = {From face-to-face meeting to video teleconferencing: Potential shifts in the meeting genre},
   journal = {Management Communication Quarterly},
   volume = {8},
   number = {4},
   pages = {395-423},
   ISSN = {0893-3189},
   year = {1995},
   type = {Journal Article}
}


@article{RN44,
   author = {Figlio, David and Rush, Mark and Yin, Lu},
   title = {Is it live or is it internet? Experimental estimates of the effects of online instruction on student learning},
   journal = {Journal of Labor Economics},
   volume = {31},
   number = {4},
   pages = {763-784},
   ISSN = {0734-306X},
   year = {2013},
   type = {Journal Article}
}

@article{RN43,
   author = {Nguyen, Tuan},
   title = {The effectiveness of online learning: Beyond no significant difference and future horizons},
   journal = {MERLOT Journal of Online Learning and Teaching},
   volume = {11},
   number = {2},
   pages = {309-319},
   year = {2015},
   type = {Journal Article}
}

@article{article,
author = {Macrae, C and Hood, Bruce and Milne, Alan and Rowe, Angela and Mason, Malia},
year = {2002},
month = {10},
pages = {460-4},
title = {Are You Looking at Me? Eye Gaze and Person Perception},
volume = {13},
journal = {Psychological science},
doi = {10.1111/1467-9280.00481}
}

@inproceedings{10.1145/503376.503386,
author = {Chen, Milton},
title = {Leveraging the Asymmetric Sensitivity of Eye Contact for Videoconference},
year = {2002},
isbn = {1581134533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/503376.503386},
doi = {10.1145/503376.503386},
abstract = {Eye contact is a natural and often essential element in the language of visual communication. Unfortunately, perceiving eye contact is difficult in most video-conferencing systems and hence limits their effectiveness. We conducted experiments to determine how accurately people perceive eye contact. We discovered that the sensitivity to eye contact is asymmetric, in that we are an order of magnitude less sensitive to eye contact when people look below our eyes than when they look to the left, right, or above our eyes. Additional experiments support a theory that people are prone to perceive eye contact, that is, we will think that someone is making eye contact with us unless we are certain that the person is not looking into our eyes. These experimental results suggest parameters for the design of videoconferencing systems. As a demonstration, we were able to construct from commodity components a simple dyadic videoconferencing prototype that supports eye contact},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {49–56},
numpages = {8},
keywords = {videoconferencing, gaze perception, eye contact},
location = {Minneapolis, Minnesota, USA},
series = {CHI '02}
}

@inproceedings{10.1145/1056808.1056995,
author = {Mukawa, Naoki and Oka, Tsugumi and Arai, Kumiko and Yuasa, Masahide},
title = {What is Connected by Mutual Gaze? User's Behavior in Video-Mediated Communication},
year = {2005},
isbn = {1595930027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1056808.1056995},
doi = {10.1145/1056808.1056995},
abstract = {Video-mediated communication systems such as teleconferencing and videophone have become popular. As with face-to-face communication, non-verbal cues such as gaze, facial expression, head orientation and gestures in visual systems play an important role. Existing systems, however, do not support mutual gaze because the lay-out of the camera and monitor is restricted. Thus, conversations using visual systems differ from those in face-to-face communication. This paper clarifies the problems of the video-mediated system, specifically for comparing the system with communication using eye-contact and with communication using no-eye-contact. This study focuses on the protocol of opening communication, e.g. establishment of a visual-audio link, person identification and confirmation of the acceptance of conversation. We conducted experiments using the two systems. Analysis of recorded video sequences revealed that the system using communication with eye-contact induced behavior similar to the system using face-to-face communication.},
booktitle = {CHI '05 Extended Abstracts on Human Factors in Computing Systems},
pages = {1677–1680},
numpages = {4},
keywords = {ethenographical approach, eye contact, gaze, video-mediated communication},
location = {Portland, OR, USA},
series = {CHI EA '05}
}

@article{10.1145/937549.937552,
author = {Grayson, David M. and Monk, Andrew F.},
title = {Are You Looking at Me? Eye Contact and Desktop Video Conferencing},
year = {2003},
issue_date = {September 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/937549.937552},
doi = {10.1145/937549.937552},
abstract = {Mutual gaze is an important conversational resource, but is difficult to provide using conventional video conferencing equipment due to the disparity between the position of the camera and the position of the eyes on the screen. Various elaborate inventions have been proposed to get around this problem but none have found wide use. The alternative explored here is that these expensive alternatives may be unnecessary. Users of conventional desktop video equipment may, under the right conditions, be able to learn to interpret what is at first sight inappropriate apparent gaze direction as signalling that the other person is "looking at me."Data are presented from two experiments where an estimator judges where a gazer is looking. The gazer may be looking either at the desktop video image of the estimator or some point to the side. Experiment 1 compared two image sizes and two camera positions. While the size of the image (352 \texttimes{} 288 pixels versus 176 \texttimes{} 144) had no significant effect on participants' ability to judge where the gazer was looking, horizontally offsetting the position of the camera inhibited performance. Experiment 2 examined the effect of reducing the image size further. The smallest image size (88 \texttimes{} 72 pixels) resulted in poorer performance than the intermediate (176 \texttimes{} 144). The results show that it is possible for users of low cost desktop video conferencing to learn to interpret gaze direction to a very high degree of accuracy if the equipment is configured optimally. The practical and theoretical implications of these results are discussed.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
pages = {221–243},
numpages = {23},
keywords = {Video conferencing, eye contact, gaze awareness}
}

@article{article2,
author = {Vertegaal, Roel and Veer, Gerrit and Vons, Harro},
year = {2000},
month = {12},
pages = {},
title = {Effects of Gaze on Multiparty Mediated Communication}
}

@ARTICLE{1090060,  author={R. {Stokes}},  journal={IEEE Transactions on Communication Technology},   title={Human Factors and Appearance Design Considerations of the Mod II PICTUREPHONE® Station Set},   year={1969},  volume={17},  number={2},  pages={318-323},  doi={10.1109/TCOM.1969.1090060}}

@inproceedings{10.1145/192844.193054,
author = {Okada, Ken-Ichi and Maeda, Fumihiko and Ichikawaa, Yusuke and Matsushita, Yutaka},
title = {Multiparty Videoconferencing at Virtual Social Distance: MAJIC Design},
year = {1994},
isbn = {0897916891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/192844.193054},
doi = {10.1145/192844.193054},
abstract = {This paper describes the design and implementation of MAJIC, a multi-party videoconferencing system that projects life-size video images of participants onto a large curved screen as if users in various locations are attending a meeting together and sitting around a table. MAJIC also supports multiple eye contact among the participants and awareness of the direction of the participants' gaze. Hence, users can carry on a discussion in a manner comparable to face-to-face meetings. We made video-tape recordings of about twenty visitors who used the prototype of MAJIC at the Nikkei Collaboration Fair in Tokyo. Our initial observations based on this experiment are also reported in this paper.},
booktitle = {Proceedings of the 1994 ACM Conference on Computer Supported Cooperative Work},
pages = {385–393},
numpages = {9},
keywords = {multiparty videoconferencing, MAJIC, groupware, tele-presence, multiple eye contact, networked realities, gaze awareness},
location = {Chapel Hill, North Carolina, USA},
series = {CSCW '94}
}

@inproceedings{10.1145/142750.142977,
author = {Ishii, Hiroshi and Kobayashi, Minoru},
title = {ClearBoard: A Seamless Medium for Shared Drawing and Conversation with Eye Contact},
year = {1992},
isbn = {0897915135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/142750.142977},
doi = {10.1145/142750.142977},
abstract = {This paper introduces a novel shared drawing medium called ClearBoard. It realizes (1) a seamless shared drawing space and (2) eye contact to support realtime and remote collaboration by two users. We devised the key metaphor: “talking through and drawing on a transparent glass window” to design ClearBoard. A prototype of ClearBoard is implemented based on the “Drafter-Mirror” architecture. This paper first reviews previous work on shared drawing support to clarify the design goals. We then examine three methaphors that fulfill these goals. The design requirements and the two possible system architectures of ClearBoard are described. Finally, some findings gained through the experimental use of the prototype, including the feature of “gaze awareness”, are discussed.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {525–532},
numpages = {8},
location = {Monterey, California, USA},
series = {CHI '92}
}

@inproceedings{10.1145/344779.344951,
author = {Matusik, Wojciech and Buehler, Chris and Raskar, Ramesh and Gortler, Steven J. and McMillan, Leonard},
title = {Image-Based Visual Hulls},
year = {2000},
isbn = {1581132085},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/344779.344951},
doi = {10.1145/344779.344951},
abstract = {In this paper, we describe an efficient image-based approach to computing and shading visual hulls from silhouette image data. Our algorithm takes advantage of epipolar geometry and incremental computation to achieve a constant rendering cost per rendered pixel. It does not suffer from the computation complexity, limited resolution, or quantization artifacts of previous volumetric approaches. We demonstrate the use of this algorithm in a real-time virtualized reality application running off a small number of video streams.},
booktitle = {Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {369–374},
numpages = {6},
keywords = {image-based rendering, computer vision, misc. rendering algorithms, constructive solid geometry},
series = {SIGGRAPH '00}
}

@article{10.1207/s15327051hci1004_2,
author = {Sellen, Abigail J.},
title = {Remote Conversations: The Effects of Mediating Talk with Technology},
year = {1995},
issue_date = {December 1995},
publisher = {L. Erlbaum Associates Inc.},
address = {USA},
volume = {10},
number = {4},
issn = {0737-0024},
url = {https://doi.org/10.1207/s15327051hci1004_2},
doi = {10.1207/s15327051hci1004_2},
abstract = {Three different videoconferencing systems for supporting multiparty, remote conversations are described and evaluated experimentally. The three systems differed by how many participants were visible at once, their spatial arrangement, and control over who was seen. Conversations using these systems were compared to same-room (Experiment 1) and audio-only (Experiment 2) conversations. Specialized speech-tracking equipment recorded the on-off patterns of speech that allowed objective measurement of structural aspects of the conversations, such as turn length, pauses, and interruptions. Questionnaires and interviews also documented participants' opinions and perceptions in the various settings.Contrary to expectation, systems in which visual cues such as selective gaze were absent produced no differences in turn-taking or in any other aspect of the structure of conversation. In fact, turn-taking was unaffected even when visual information was completely absent. Overall, only the same-room condition showed any significant differences from any other condition; people in the same room produced more interruptions and fewer formal handovers of the floor than in any of the technology-mediated conditions. In this respect, the audio-only and video systems examined in these studies were equivalent. However, analyses of participants' perceptions showed that participants felt that visual access in mediated conversations was both important and beneficial in conversation. Further, there were indications that the particular design of the different video systems did affect some aspects of conversational behavior, such as the ability to hold side and parallel conversations.},
journal = {Hum.-Comput. Interact.},
month = dec,
pages = {401–444},
numpages = {44}
}

@article{10.1145/1015706.1015805,
author = {Matusik, Wojciech and Pfister, Hanspeter},
title = {3D TV: A Scalable System for Real-Time Acquisition, Transmission, and Autostereoscopic Display of Dynamic Scenes},
year = {2004},
issue_date = {August 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1015706.1015805},
doi = {10.1145/1015706.1015805},
abstract = {Three-dimensional TV is expected to be the next revolution in the history of television. We implemented a 3D TV prototype system with real-time acquisition, transmission, and 3D display of dynamic scenes. We developed a distributed, scalable architecture to manage the high computation and bandwidth demands. Our system consists of an array of cameras, clusters of network-connected PCs, and a multi-projector 3D display. Multiple video streams are individually encoded and sent over a broadband network to the display. The 3D display shows high-resolution (1024 \texttimes{} 768) stereoscopic color images for multiple viewpoints without special glasses. We implemented systems with rear-projection and front-projection lenticular screens. In this paper, we provide a detailed overview of our 3D TV system, including an examination of design choices and tradeoffs. We present the calibration and image alignment procedures that are necessary to achieve good image quality. We present qualitative results and some early user feedback. We believe this is the first real-time end-to-end 3D TV system with enough views and resolution to provide a truly immersive 3D experience.},
journal = {ACM Trans. Graph.},
month = aug,
pages = {814–824},
numpages = {11},
keywords = {image-based rendering, projector arrays, Autostereoscopic displays, multiview displays, lightfields, camera arrays}
}
@inproceedings{10.1145/166266.166289,
author = {Isaacs, Ellen A. and Tang, John C.},
title = {What Video Can and Can't Do for Collaboration: A Case Study},
year = {1993},
isbn = {0897915968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/166266.166289},
doi = {10.1145/166266.166289},
booktitle = {Proceedings of the First ACM International Conference on Multimedia},
pages = {199–206},
numpages = {8},
keywords = {video conferencing, user interfaces, remote collaboration, computer-supported cooperative work},
location = {Anaheim, California, USA},
series = {MULTIMEDIA '93}
}

@inproceedings{10.1145/1186562.1015766,
author = {Zitnick, C. Lawrence and Kang, Sing Bing and Uyttendaele, Matthew and Winder, Simon and Szeliski, Richard},
title = {High-Quality Video View Interpolation Using a Layered Representation},
year = {2004},
isbn = {9781450378239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1186562.1015766},
doi = {10.1145/1186562.1015766},
abstract = {The ability to interactively control viewpoint while watching a video is an exciting application of image-based rendering. The goal of our work is to render dynamic scenes with interactive viewpoint control using a relatively small number of video cameras. In this paper, we show how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms. Once these video streams have been processed, we can synthesize any intermediate view between cameras at any time, with the potential for space-time manipulation.In our approach, we first use a novel color segmentation-based stereo algorithm to generate high-quality photoconsistent correspondences across all camera views. Mattes for areas near depth discontinuities are then automatically extracted to reduce artifacts during view synthesis. Finally, a novel temporal two-layer compressed representation that handles matting is developed for rendering at interactive rates.},
booktitle = {ACM SIGGRAPH 2004 Papers},
pages = {600–608},
numpages = {9},
keywords = {Image-Based Rendering, Computer Vision, Dynamic Scenes},
location = {Los Angeles, California},
series = {SIGGRAPH '04}
}

@article{argyle_cook_cramer_1994, title={Gaze and Mutual Gaze}, volume={165}, DOI={10.1017/S0007125000073980}, number={6}, journal={British Journal of Psychiatry}, publisher={Cambridge University Press}, author={Argyle, Michael and Cook, Mark and Cramer, Duncan}, year={1994}, pages={848–850}}

@article{article9,
author = {Petit, Benjamin and Lesage, Jean-Denis and Ménier, Clément and Allard, Jeremie and Franco, Jean-Sebastien and Raffin, Bruno and Boyer, Edmond and Faure, Francois},
year = {2010},
month = {08},
pages = {},
title = {Multi-Camera Real-Time 3D Modeling for Telepresence and Remote Collaboration},
volume = {2010},
journal = {International Journal of Digital Multimedia Broadcasting},
doi = {10.1155/2010/247108}
}

@inproceedings {a12,
booktitle = {Vision, Modeling, and Visualization (2011)},
editor = {Peter Eisert and Joachim Hornegger and Konrad Polthier},
title = {{FreeCam: A Hybrid Camera System for Interactive Free-Viewpoint Video}},
author = {Kuster, Claudia and Popa, Tiberiu and Zach, Christopher and Gotsman, Craig and Gross, Markus},
year = {2011},
publisher = {The Eurographics Association},
ISBN = {978-3-905673-85-2},
DOI = {10.2312/PE/VMV/VMV11/017-024}
}

@article{10.1145/2366145.2366193,
author = {Kuster, Claudia and Popa, Tiberiu and Bazin, Jean-Charles and Gotsman, Craig and Gross, Markus},
title = {Gaze Correction for Home Video Conferencing},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2366145.2366193},
doi = {10.1145/2366145.2366193},
abstract = {Effective communication using current video conferencing systems is severely hindered by the lack of eye contact caused by the disparity between the locations of the subject and the camera. While this problem has been partially solved for high-end expensive video conferencing systems, it has not been convincingly solved for consumer-level setups. We present a gaze correction approach based on a single Kinect sensor that preserves both the integrity and expressiveness of the face as well as the fidelity of the scene as a whole, producing nearly artifact-free imagery. Our method is suitable for mainstream home video conferencing: it uses inexpensive consumer hardware, achieves real-time performance and requires just a simple and short setup. Our approach is based on the observation that for our application it is sufficient to synthesize only the corrected face. Thus we render a gaze-corrected 3D model of the scene and, with the aid of a face tracker, transfer the gaze-corrected facial portion in a seamless manner onto the original image.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {174},
numpages = {6},
keywords = {video conferencing, gaze correction, depth camera}
}

@article{article0,
author = {Al-Samarraie, Hosam},
year = {2019},
month = {07},
pages = {},
title = {A Scoping Review of Videoconferencing Systems in Higher Education: Learning Paradigms, Opportunities, and Challenges},
volume = {20},
journal = {International Review of Research in Open and Distance Learning},
doi = {10.19173/irrodl.v20i4.4037}
}

@book{RN4,
   author = {Duchowski, Andrew T and Duchowski, Andrew T},
   title = {Eye tracking methodology: Theory and practice},
   publisher = {Springer},
   ISBN = {3319578839},
   year = {2017},
   type = {Book}
}

@article{RN5,
   author = {Morimoto, Carlos H and Mimica, Marcio RM},
   title = {Eye gaze tracking techniques for interactive applications},
   journal = {Computer vision and image understanding},
   volume = {98},
   number = {1},
   pages = {4-24},
   ISSN = {1077-3142},
   year = {2005},
   type = {Journal Article}
}

@inproceedings{RN6,
   author = {Ohno, Takehiko and Mukawa, Naoki and Yoshikawa, Atsushi},
   title = {FreeGaze: a gaze tracking system for everyday gaze interaction},
   booktitle = {Proceedings of the 2002 symposium on Eye tracking research and applications},
   pages = {125-132},
   type = {Conference Proceedings}
}

@misc{RN11,
   volume = {2021},
   author={NVidia},
   title = {GANs Improve Video Conferencing with Maxine},
   url = {https://blogs.nvidia.com/blog/2020/10/05/ganvideo-conferencing-maxine/},
   year = {2020},
   type = {Web Page}
}

